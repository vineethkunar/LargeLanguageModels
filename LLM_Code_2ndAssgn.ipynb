{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vineethkunar/LargeLanguageModels/blob/main/LLM_Code_2ndAssgn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required NLP and evaluation libraries from Hugging Face\n",
        "!pip install -q transformers datasets evaluate\n",
        "# Suppress deprecation warnings to keep the notebook output clean\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "# Import core data manipulation libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Import text processing libraries for NLP\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# Import visualization libraries\n",
        "from wordcloud import WordCloud\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "# Import scikit-learn utilities for feature extraction, evaluation, and data preparation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Import system utilities\n",
        "import os\n",
        "# Import Hugging Face Transformers for BERT model\n",
        "from transformers import BertForSequenceClassification\n",
        "# Import PyTorch for tensor operations and GPU support\n",
        "import torch\n",
        "# Import Hugging Face Datasets for preparing inputs\n",
        "from datasets import Dataset\n",
        "# Import tokenizer, model, and training tools from Transformers\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "# Import evaluation tools for model performance\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "px6qbKtlMMfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the training dataset from a GitHub URL into a DataFrame\n",
        "Sub_Train_Df = pd.read_csv(\"https://raw.githubusercontent.com/vineethkunar/LargeLanguageModels/refs/heads/main/train.csv\")\n",
        "# Load the test dataset from a GitHub URL into a DataFrame\n",
        "Sub_Test_Df = pd.read_csv(\"https://raw.githubusercontent.com/vineethkunar/LargeLanguageModels/refs/heads/main/test.csv\")"
      ],
      "metadata": {
        "id": "5coBuZclQa9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first five rows of the training DataFrame to inspect the data\n",
        "print(Sub_Train_Df.head())"
      ],
      "metadata": {
        "id": "4H5wWzgVMSUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate summary statistics for numerical columns in the training dataset\n",
        "Sub_Train_Df.describe()"
      ],
      "metadata": {
        "id": "1ZuGaItaMb8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the number of rows and columns in the training dataset\n",
        "Sub_Train_rows, Sub_Train_Cols = Sub_Train_Df.shape\n",
        "# Print the dataset dimensions in a readable format\n",
        "print(f\"The shape of the original dataset is {Sub_Train_rows} reviews with {Sub_Train_Cols} columns.\")"
      ],
      "metadata": {
        "id": "Hjg0ShS-MgJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the 'Id' column from the training dataset\n",
        "Sub_Train_Df = Sub_Train_Df.drop(columns=['Id']\n",
        "# Remove the 'Id' column from the test dataset\n",
        "Sub_Test_Df = Sub_Test_Df.drop(columns=['Id'])"
      ],
      "metadata": {
        "id": "4fvIUBEeriOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the count of missing (null) values in each column of the training dataset\n",
        "Sub_Train_Df.isna().sum()"
      ],
      "metadata": {
        "id": "Wtq-0Of8Mi2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the data types and non-null counts of each column in the training dataset\n",
        "Sub_Train_Df.info()"
      ],
      "metadata": {
        "id": "UHlGq6c0MmDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK resources for text preprocessing\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Create a lemmatizer for word normalization\n",
        "subject_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Load English stopwords to remove common filler words\n",
        "filtered_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "# Function to clean and prepare text for subject classification\n",
        "def prepare_subject_text(raw_input):\n",
        "    \"\"\"\n",
        "    Cleans and normalizes the input text to make it suitable for subject classification.\n",
        "    It performs case normalization, removes non-alphabetic characters, filters out stopwords,\n",
        "    and applies lemmatization to reduce words to their base form.\n",
        "\n",
        "    Parameters:\n",
        "    raw_input (str): A raw string of text representing the subject-related comment or description.\n",
        "\n",
        "    Returns:\n",
        "    str: A cleaned and lemmatized version of the input text.\n",
        "    \"\"\"\n",
        "    cleaned_text = raw_input.lower()\n",
        "    cleaned_text = re.sub(r'[^a-z\\s]', '', cleaned_text)\n",
        "    tokens = cleaned_text.split()\n",
        "    meaningful_tokens = [subject_lemmatizer.lemmatize(word) for word in tokens if word not in filtered_stopwords]\n",
        "    return ' '.join(meaningful_tokens)\n",
        "\n",
        "# Apply the text cleaning function to the training comments\n",
        "Sub_Train_Df['Processed_Subject_Text'] = Sub_Train_Df['Comment'].apply(prepare_subject_text)\n",
        "\n",
        "# Show the original and processed columns side-by-side\n",
        "Sub_Train_Df[['Comment', 'Processed_Subject_Text']]\n"
      ],
      "metadata": {
        "id": "7N0oamUqMuSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a new column that stores the character length of each original comment\n",
        "Sub_Train_Df['len_orgcomment'] = Sub_Train_Df['Comment'].apply(len)"
      ],
      "metadata": {
        "id": "9xAn29zaNCDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how many rows in the dataset are exact duplicates\n",
        "print(f\"Number of duplicate rows: {Sub_Train_Df.duplicated().sum()}\")\n",
        "# Remove duplicate rows from the training DataFrame\n",
        "Sub_Train_Df = Sub_Train_Df.drop_duplicates()\n",
        "# Print the new shape of the dataset after removing duplicates\n",
        "print(f\"Shape after removing duplicates: {Sub_Train_Df.shape}\")"
      ],
      "metadata": {
        "id": "2mhYomdFNItE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first five rows of the cleaned and updated training dataset\n",
        "Sub_Train_Df.head()"
      ],
      "metadata": {
        "id": "33VkDDLqNOdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the list of column names in the dataset after preprocessing steps\n",
        "Sub_Train_Df.columns"
      ],
      "metadata": {
        "id": "M84ORLcfNSMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the distribution of classes in the target 'Topic' column\n",
        "Sub_count = Sub_Train_Df['Topic'].value_counts()\n",
        "# Print the count of each label (subject/topic)\n",
        "print(\"Topic Distribution:\\n\", Sub_count)"
      ],
      "metadata": {
        "id": "Ujez8GUuNmUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the distribution of subject topics in the dataset\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='Topic', data=Sub_Train_Df)\n",
        "plt.title(\"TOpic Distribution\")\n",
        "plt.xlabel(\"Topic\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OX6VACnENr3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a new column with the word count of each preprocessed comment\n",
        "Sub_Train_Df['len_ppcomment'] = Sub_Train_Df['Processed_Subject_Text'].apply(lambda x: len(x.split()))"
      ],
      "metadata": {
        "id": "cywnRGxsNwZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the distribution of word counts in comments used for subject classification\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(Sub_Train_Df['len_ppcomment'], bins=30, kde=True)\n",
        "plt.title(\"Word Count Distribution in Subject Classification Comments\")\n",
        "plt.xlabel(\"Number of Words in Comment\")\n",
        "plt.ylabel(\"Number of Comments\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b14GP1l2N47h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the target subject labels as integers for model compatibility\n",
        "subject_label_encoder = LabelEncoder()\n",
        "Sub_Train_Df[\"Encoded_Label\"] = subject_label_encoder.fit_transform(Sub_Train_Df[\"Topic\"])\n",
        "Sub_Test_Df[\"Encoded_Label\"] = subject_label_encoder.transform(Sub_Test_Df[\"Topic\"])\n",
        "# Assign the updated DataFrames to new clearly named variables for training and testing\n",
        "Subject_Train_Data = Sub_Train_Df\n",
        "Subject_Test_Data = Sub_Test_Df\n",
        "# Convert the DataFrames into Hugging Face Dataset objects for model input\n",
        "Subject_Train_Set = Dataset.from_pandas(Subject_Train_Data)\n",
        "Subject_Test_Set = Dataset.from_pandas(Subject_Test_Data)"
      ],
      "metadata": {
        "id": "_EuFrw2SOp6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the pre-trained model and tokenizer for subject classification\n",
        "subject_model_name = \"bert-base-uncased\"\n",
        "subject_tokenizer = AutoTokenizer.from_pretrained(subject_model_name)"
      ],
      "metadata": {
        "id": "6wDLc6ayO-jU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to tokenize and encode the input comments for the model\n",
        "def encode_subject_batch(batch):\n",
        "    tokens = subject_tokenizer(\n",
        "        batch[\"Comment\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "    tokens[\"labels\"] = batch[\"Encoded_Label\"]\n",
        "    return tokens\n",
        "\n",
        "# Apply the tokenizer to the training and test datasets\n",
        "Subject_Train_Set = Subject_Train_Set.map(encode_subject_batch, batched=True)\n",
        "Subject_Test_Set = Subject_Test_Set.map(encode_subject_batch, batched=True)\n"
      ],
      "metadata": {
        "id": "PMTB1Y36PBaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained BERT model for subject classification with 3 output labels\n",
        "subject_model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=3)"
      ],
      "metadata": {
        "id": "IhA19LMFPCtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training parameters for subject classification\n",
        "subject_batch_size = 64\n",
        "subject_logging_steps = len(Subject_Train_Set) // subject_batch_size\n",
        "subject_model_id = subject_model_name.split(\"/\")[-1]"
      ],
      "metadata": {
        "id": "oNzyzWr6QRXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Disable Weights & Biases logging during training\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "# Set up training arguments for the subject classification model\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_train_epochs=3,\n",
        "    fp16=True,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "id": "Z7U0KxXbQTlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the evaluation metric function for subject classification\n",
        "subject_accuracy_metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_subject_metrics(prediction_batch):\n",
        "    \"\"\"\n",
        "    Computes the accuracy score for subject classification predictions.\n",
        "    It takes the model's raw output scores and true subject labels, converts scores\n",
        "    into predicted class indices, and evaluates them using the accuracy metric.\n",
        "\n",
        "    Parameters:\n",
        "    prediction_batch (tuple): A tuple containing model output scores and true subject labels.\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary containing the accuracy score.\n",
        "    \"\"\"\n",
        "    model_scores, true_subjects = prediction_batch\n",
        "    predicted_subjects = torch.argmax(torch.tensor(model_scores), dim=-1)\n",
        "    return subject_accuracy_metric.compute(predictions=predicted_subjects, references=true_subjects)"
      ],
      "metadata": {
        "id": "2v7HOdedRe8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Trainer for subject classification and start model training\n",
        "subject_trainer = Trainer(\n",
        "    model=subject_model,\n",
        "    args=subject_training_args,\n",
        "    train_dataset=Subject_Train_Set,\n",
        "    eval_dataset=Subject_Test_Set,\n",
        "    compute_metrics=compute_subject_metrics)\n",
        "# Begin training the model\n",
        "subject_trainer.train()"
      ],
      "metadata": {
        "id": "gk_lE6a9RhPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the list of original subject class names from the label encoder\n",
        "print(\"Class Names:\", subject_label_encoder.classes_)"
      ],
      "metadata": {
        "id": "888Cvw4OvFUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the trained model on the subject classification test set\n",
        "subject_eval_results = subject_trainer.evaluate()\n",
        "# Print the evaluation metrics in a readable format\n",
        "print(\"Evaluation Results:\")\n",
        "for metric_name, metric_value in subject_eval_results.items():\n",
        "    print(f\"{metric_name}: {metric_value:.4f}\")"
      ],
      "metadata": {
        "id": "gpw5DA_JfhNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions on the test set using the trained subject classification model\n",
        "subject_predictions = subject_trainer.predict(Subject_Test_Set)\n",
        "# Extract true labels and predicted labels\n",
        "true_subject_labels = subject_predictions.label_ids\n",
        "predicted_subject_labels = torch.argmax(torch.tensor(subject_predictions.predictions), axis=1).numpy()\n",
        "# Compute and display the confusion matrix\n",
        "conf_matrix = confusion_matrix(true_subject_labels, predicted_subject_labels)\n",
        "conf_matrix_display = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
        "conf_matrix_display.plot(cmap=\"Blues\", values_format=\"d\")\n",
        "plt.title(\"Confusion Matrix for Subject Classification\")\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j3K_ZK1eflOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate and print the classification report for subject prediction\n",
        "subject_report = classification_report(true_subject_labels, predicted_subject_labels, digits=4)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(subject_report)"
      ],
      "metadata": {
        "id": "ZXAsKWSPoTN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract original comment texts from the test dataset\n",
        "sample_subject_texts = Subject_Test_Set[\"Comment\"]\n",
        "# Display 5 sample predictions with their true and predicted labels\n",
        "print(\"\\nSample Predictions on Test Set :\\n\")\n",
        "for i in range(5):\n",
        "    print(f\"Text: {sample_subject_texts[i]}\")\n",
        "    print(f\"Predicted Label: {predicted_subject_labels[i]}, True Label: {true_subject_labels[i]}\")\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "id": "nQhgGBfVfn3-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}