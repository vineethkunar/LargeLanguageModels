This repository contains a complete pipeline for subject classification based on text data using pre-trained Transformer models. The project begins with data loading, cleaning, and preprocessing, including text normalization, stopword removal, and lemmatization. Exploratory data analysis (EDA) is conducted to understand label distribution and comment lengths. The cleaned data is tokenized using Hugging Face’s AutoTokenizer, and labels are encoded numerically to prepare for model training. A BERT-based model (bert-base-uncased) is fine-tuned using Hugging Face’s Trainer API with appropriate training arguments. The training process includes gradient accumulation, logging, and evaluation metrics like accuracy. After training, model performance is evaluated using a confusion matrix and classification report. Predictions are demonstrated on sample comments to show the model's practical use. The code is modular, well-documented, and can be easily adapted to similar multi-class text classification tasks.

All model training and evaluation steps are reproducible and use open-source libraries. Users are encouraged to adapt the dataset and model type (e.g., RoBERTa) as needed. This project is licensed under the MIT License, which allows reuse, modification, and distribution with proper attribution. Under this license, you are free to use the code for both personal and commercial purposes, but any derivative work must retain the original license and copyright notice. No warranties are provided, and the code is provided "as-is".
